{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17ChGvu5N4BX"
      },
      "source": [
        "# Transfer Learning from Tennis to Pong\n",
        "\n",
        "**Neuromatch Reinforcement Learning Project 2023**\n",
        "\n",
        "__Authors:__ Amirozhan Dehghani, Parmida Pezeshki, Alireza Astane, Sanchit Baweja\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOBYuRbyjiQd"
      },
      "source": [
        "# Things to work on\n",
        "\n",
        "1.   Find the best parameters to train the RL on Pong\n",
        "2.   Why is the loss so small?\n",
        "3.   ~~Fix the bug when training on Pong Enviroment~~ (**Resolved**)\n",
        "4.   Find the best parameters to train the RL on Pong\n",
        "5.   Clean-up the code for easy training on either enviroments\n",
        "6.   Write the code to transfer the weights from Pong to Tennis enviroment\n",
        "7.   Train the model on Pong and transfer it to train on Tennis\n",
        "8.   Save the results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FEfjuNBNdGN"
      },
      "outputs": [],
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install -y swig build-essential python-dev python3-dev > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLs1HnD3PgpM",
        "outputId": "318eca86-5129-40c1-efb9-4ae8e4984092"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.4/178.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not build wheels for box2d-py, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m853.1/853.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# @title Install dependencies\n",
        "!pip install rarfile --quiet\n",
        "!pip install stable-baselines3[extra] --quiet\n",
        "!pip install ale-py --quiet\n",
        "!pip install gym[box2d] --quiet\n",
        "!pip install pyvirtualdisplay --quiet\n",
        "!pip install pyglet --quiet\n",
        "!pip install pygame --quiet\n",
        "!pip install minigrid --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLuvyCtFPizQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2467aa3d-8e28-4ff6-bf4c-e9d7d0c1cc2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.5.0 (SDL 2.28.0, Python 3.10.6)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "from statistics import mean as MEAN\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import make_grid\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import math\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import base64\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "import gymnasium\n",
        "sys.modules[\"gym\"] = gymnasium\n",
        "from itertools import count\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "import gymnasium as gym\n",
        "from gym import spaces\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1LZPxuDckUx"
      },
      "source": [
        "# Parameter Selection for Model Training\n",
        "\n",
        "\n",
        "1. Try as many parameter combinations as possible.\n",
        "  * We might need to grayscale enviroment\n",
        "  * We might need batch normalization after each Conv layer\n",
        "\n",
        "3. The parameters, along with the final reward and loss, will be saved in a CSV file.\n",
        "  * If you want to submit your experiment as an official entry, change the `SAVE` variable to **\"Shared\"** so we can evaluate the best parameters.\n",
        "\n",
        "\n",
        "I found a similar code, look at their hyperparameters [GitHub - rl-atari-tennis](https://github.com/leerumor/rl-atari-tennis).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OLD PARAM**\n",
        "* ~~Parmida: lr= 0.025 , tau= 0.005, batch-size=512, num_episdodes = 250~~ (not good)  \n",
        "* ~~Shreya:  lr= 0.025, tau= 0.05, batch-size=512, num_episdodes = 250~~ (not good)\n",
        "  * Shreya will also run with 3 channel instead of 4 channels  \n",
        "* ~~Sanchit: lr=0.0025  tau=0.05, batch_size = 512, num_episdodes = 250~~ (not good)\n",
        "* Alireza: lr=0.0025, tau= 0.005, batch_size = 512, num_episdodes = 250\n",
        "* Amirozhan: lr= 0.025, tau= 0.5, batch_size = 512, num_episdodes = 250 **(best one so far)**"
      ],
      "metadata": {
        "id": "YtDkgNyUlUlI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek3SjHOiZTqn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a983c9-9a4e-41f4-89b0-f5879e0f7765"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JJOe0Sqasjc"
      },
      "outputs": [],
      "source": [
        "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
        "# GAMMA is the discount factor as mentioned in the previous section\n",
        "# EPS_START is the starting value of epsilon\n",
        "# EPS_END is the final value of epsilon\n",
        "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
        "# TAU is the update rate of the target network\n",
        "# LR is the learning rate of the ``AdamW`` optimizer\n",
        "# SAVE to choose whether to save the file locally or in a shared folder\n",
        "\n",
        "SAVE = \"Locally\" #@param [\"Locally\",\"Shared\"]\n",
        "Author = \"Amirozhan\" #@param [\"Amirozhan\", \"Sanchit\", \"Shreya\", \"Parmida\", \"Alireza\", \"Noureldin\"] {allow-input: true}\n",
        "num_episodes = 1000 #@param [50,100,500,1000] {type:\"raw\", allow-input: true}\n",
        "GAMES = \"Pong\" #@param [\"Tennis\",\"Pong\",\"CarPole-v0\"]\n",
        "BATCH_SIZE = 32 #@param [128,256,512,1024] {type:\"raw\", allow-input: true}\n",
        "GAMMA = 0.99 #@param [0.99,1.5,2.0,2.5] {type:\"raw\", allow-input: true}\n",
        "DECAY_TYPE = \"Exponential\" #@param [\"Exponential\",\"Linear\",\"Memorical\"]\n",
        "EPS_START = 1 #@param [0.5,0.9,2,2.5,3] {type:\"raw\", allow-input: true}\n",
        "EPS_END = 0.01 #@param [0.01,0.04,0.05,0.1] {type:\"raw\", allow-input: true}\n",
        "EPS_DECAY = 700 #@param [100,500,1000,1500] {type:\"raw\", allow-input: true}\n",
        "TAU = 1e-3 #@param [0.00005,0.0005,0.005,0.05] {type:\"raw\", allow-input:\n",
        "LR = 1e-4 #@param [1e-6,1e-5,1e-4,1e-3] {type:\"raw\", allow-input: true}\n",
        "MIN_MEMORY_LEN = 10000 #@param\n",
        "MEMORY_SIZE = 100000 #@param\n",
        "STEP_SIZE = 300 #@param {type:\"raw\", allow-input: true}\n",
        "plot_every = 20 #@param [5,20,50,100] {type:\"raw\", allow-input: true}\n",
        "numOfLastActions = 5 #@param [5,10,15,20] {type:\"raw\", allow-input: true}\n",
        "UPDATE_FREQ = 1000#@param [5,10,15,20] {type:\"raw\", allow-input: true}\n",
        "seed = 42 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "my_param={\n",
        "    \"Empty\" : \"\",\n",
        "    \"time\": None,\n",
        "    \"EPOCHS\": [num_episodes],\n",
        "    \"MEMORY_SIZE\": [MEMORY_SIZE],\n",
        "    \"MIN_MEMORY_LEN\":[MIN_MEMORY_LEN],\n",
        "    \"Final_Loss\" : None,\n",
        "    \"Final_Reward\" : None,\n",
        "    \"UPDATE_FREQ\" : [UPDATE_FREQ],\n",
        "    \"STEP_SIZE\": [STEP_SIZE],\n",
        "    \"Author\" : [Author],\n",
        "    \"Device\" : [device],\n",
        "    \"BATCH_SIZE\" : [BATCH_SIZE],\n",
        "    \"GAMES\" : [GAMES],\n",
        "    \"GAMMA\" : [GAMMA],\n",
        "    \"DECAY_TYPE\" : [DECAY_TYPE],\n",
        "    \"EPS_START\": [EPS_START],\n",
        "    \"EPS_END\" : [EPS_END],\n",
        "    \"EPS_DECAY\" : [EPS_DECAY],\n",
        "    \"TAU\":[TAU],\n",
        "    \"LR\":[LR],\n",
        "          }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrUm3RQcO8iN"
      },
      "outputs": [],
      "source": [
        "def _to_save (SAVE):\n",
        "  filename = \"summary.csv\"\n",
        "  if SAVE == \"Locally\":\n",
        "      save_directory = \"./Saved_Params\"\n",
        "      os.makedirs(save_directory, exist_ok=True)\n",
        "      filepath = os.path.join(save_directory, filename)\n",
        "  elif SAVE == \"Shared\":\n",
        "      drive.mount('/content/drive')\n",
        "      folder_path = '/content/drive/MyDrive/NeuroMartch_Academy '\n",
        "      if not os.path.exists(folder_path):\n",
        "        raise ValueError(\"Please move the NeuroMartch_Academy to the MyDrive\")\n",
        "      filepath = os.path.join(folder_path, filename)\n",
        "  return filepath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRbTYbusNr3o"
      },
      "outputs": [],
      "source": [
        "def Save_as_csv(SAVE,summary_data):\n",
        "    filepath = _to_save(SAVE)\n",
        "    summary_df = pd.DataFrame.from_dict(summary_data)\n",
        "\n",
        "    if os.path.exists(filepath):\n",
        "        # If the file exists, load the existing DataFrame and append the new row\n",
        "        existing_df = pd.read_csv(filepath, index_col=0)\n",
        "        updated_df = existing_df.append(summary_df, ignore_index=True)\n",
        "\n",
        "    else:\n",
        "\n",
        "        updated_df = summary_df\n",
        "\n",
        "    updated_df.to_csv(filepath, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clBZryQ3xlbM"
      },
      "source": [
        "**Memory Replay We might use it?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5iVmOOHsIpO"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n",
        "            # so its important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n",
        "        self._skip = skip\n",
        "\n",
        "    def reset(self):\n",
        "        return self.env.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info, _ = self.env.step(action)\n",
        "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame\n",
        "        # doesn't matter\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        Expects inputs to be of shape num_channels x height x width.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0] * k, shp[1], shp[2]), dtype=np.uint8)\n",
        "\n",
        "    def reset(self,**kwargs):\n",
        "        ob = self.env.reset(**kwargs)\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob()\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info,_ = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # careful! This undoes the memory optimization, use\n",
        "        # with smaller replay buffers only.\n",
        "        return np.array(observation).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\"\"\"\n",
        "        self._frames = frames\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = np.concatenate(self._frames, axis=0)\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._frames)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._frames[i]\n",
        "\n",
        "\n",
        "class PyTorchFrame(gym.ObservationWrapper):\n",
        "    \"\"\"Image shape to num_channels x height x width\"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        super(PyTorchFrame, self).__init__(env)\n",
        "        shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(shape[-1], shape[0], shape[1]), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.rollaxis(observation, 2)\n",
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\n",
        "        Expects inputs to be of shape height x width x num_channels\n",
        "        \"\"\"\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.width = 84\n",
        "        self.height = 84\n",
        "        self.observation_space = spaces.Box(low=0, high=255,\n",
        "                                            shape=(self.height, self.width, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "        return frame[:, :, None]\n",
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
        "        return np.sign(reward)\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        self.noop_action = 0\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset(**kwargs)\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = self.unwrapped.np_random.integers(1, self.noop_max + 1)  # pylint: disable=E1101\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _ , _= self.env.step(self.noop_action)\n",
        "            if done:\n",
        "                obs = self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)"
      ],
      "metadata": {
        "id": "cBa-9ETWoyzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9zWCL60c5UU"
      },
      "outputs": [],
      "source": [
        "def resize_frame(state):\n",
        "    state = state[30:-12,5:-4]\n",
        "    state = np.average(state,axis = 2) #Grayscaling the depth channel\n",
        "    state = cv2.resize(state,(84,84),interpolation = cv2.INTER_NEAREST)\n",
        "    state = np.ascontiguousarray(state, dtype=np.float32) / 255\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_epsilon(DECAY_TYPE,i_episode):\n",
        "  if DECAY_TYPE == \"Exponential\":\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-2. * i_episode / EPS_DECAY)\n",
        "  elif DECAY_TYPE == \"Linear\":\n",
        "     eps_threshold = max(EPS_START - (EPS_START - EPS_END) * (i_episode / EPS_DECAY),EPS_END)\n",
        "  #new\n",
        "  elif DECAY_TYPE == \"Memorical\":\n",
        "     eps_threshold = max(1/(getVarOfLastActions() + 1/EPS_START),EPS_END)  # 5 can be changed\n",
        "  return eps_threshold\n"
      ],
      "metadata": {
        "id": "dczWTXhWzCKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#new  #its dirty but im realy not familiar with deques\n",
        "\n",
        "def getVarOfLastActions():\n",
        "  lastActions = np.zeros(numOfLastActions)\n",
        "  try:\n",
        "    for i in range(numOfLastActions):\n",
        "      lastActions[i] = int(memory.memory[-i][1][0][0])\n",
        "  finally:\n",
        "    return np.var(lastActions)\n",
        "\"\"\"\n",
        "\n",
        "def getVarOfLastActions():\n",
        "  start = time.time()\n",
        "  last_actions = torch.Tensor([transition.action[0] for transition in memory.memory][-numOfLastActions:])\n",
        "  if len(last_actions) == numOfLastActions:\n",
        "    end = time.time()\n",
        "    print(end-start)\n",
        "    return torch.var(last_actions,dim=0,keepdim=True)\n",
        "  else:\n",
        "    return 1\n",
        "  \"\"\"\n"
      ],
      "metadata": {
        "id": "10ORG-SYT84c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "5c9cbb9b-aff1-4b75-bc84-ae9d71637569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\ndef getVarOfLastActions():\\n  start = time.time()\\n  last_actions = torch.Tensor([transition.action[0] for transition in memory.memory][-numOfLastActions:])\\n  if len(last_actions) == numOfLastActions:\\n    end = time.time()\\n    print(end-start)\\n    return torch.var(last_actions,dim=0,keepdim=True)\\n  else:\\n    return 1\\n  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = f\"PongNoFrameskip-v4\"\n",
        "env = gym.make(env_name,render_mode=\"rgb_array\")\n",
        "print('State shape: ', env.observation_space.shape)\n",
        "print('Number of actions: ', env.action_space.n)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "env.seed(seed)\n",
        "env = NoopResetEnv(env, noop_max=30)\n",
        "env = MaxAndSkipEnv(env, skip=4)\n",
        "env = EpisodicLifeEnv(env)\n",
        "env = FireResetEnv(env)\n",
        "env = WarpFrame(env)\n",
        "env = PyTorchFrame(env)\n",
        "env = ClipRewardEnv(env)\n",
        "env = FrameStack(env, 4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMZ9RvhqpPx5",
        "outputId": "1d552d5f-ee10-40ec-f5c0-40c76a5cde2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State shape:  (210, 160, 3)\n",
            "Number of actions:  6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doS689izUnnd"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(n_observations, 16, kernel_size=(8,8),stride=4)\n",
        "        #self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32,64,kernel_size=(4,4),stride = 2)\n",
        "        #self.bn2 = nn.BatchNorm2d(64)\n",
        "        #self.conv3 = nn.Conv2d(64,64,kernel_size=(3,3),stride =1)\n",
        "        #self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = nn.Linear(3136, 256)\n",
        "        self.layer2 = nn.Linear(256, n_actions)\n",
        "    def forward(self, x):\n",
        "\n",
        "        #x = x.unsqueeze(1) #Used for GrayScale\n",
        "        #x = x.permute(0,3,1,2)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        #x = F.relu(self.conv3(x))\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = F.relu(self.layer1(x))\n",
        "        out = self.layer2(x)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR0GkRBTZJW0"
      },
      "outputs": [],
      "source": [
        "n_actions = env.action_space.n\n",
        "\n",
        "\n",
        "#n_observations = 3 #change when running RGB\n",
        "n_observations = 4\n",
        "\n",
        "\n",
        "policy_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "#optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
        "# Add AdamW: Small error in the implementation of the Adam.\n",
        "#optimizer = optim.SGD(policy_net.parameters(), lr=LR)\n",
        "optimizer = optim.RMSprop(policy_net.parameters(), lr=LR, alpha=0.9, eps=1e-02, momentum=0.0)\n",
        "# make Gamma larger\n",
        "#Should work with no scheduler\n",
        "scheduler = StepLR(optimizer, step_size=STEP_SIZE, gamma=0.1)\n",
        "\n",
        "memory = ReplayMemory(MEMORY_SIZE)\n",
        "\n",
        "# Take output of the policy net state, devide it by TAU, apply softmax. and then decrease the TAU\n",
        "def select_action(state):\n",
        "    sample = random.random()\n",
        "\n",
        "    eps_threshold= get_epsilon(DECAY_TYPE,i_episode)\n",
        "\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            return policy_net(state).argmax().view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybuPC59N1X-K",
        "outputId": "3137df24-6982-4901-9eac-4a530ba0bac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jvNVnMO1bgu",
        "outputId": "8baaadeb-3c77-4d58-82b1-332fae327ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(n_observations, 32, kernel_size=(8,8),stride=4)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.conv2 = nn.Conv2d(32,64,kernel_size=(4,4),stride = 2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.conv3 = nn.Conv2d(64,64,kernel_size=(3,3),stride =1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.layer1 = nn.Linear(3136, 512)\n",
        "        self.layer2 = nn.Linear(512, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = F.relu(self.layer1(x))\n",
        "        out = self.layer2(x)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "SjuJ0_0K1cd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DQN(n_observations, n_actions)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Print the model summary\n",
        "summary(model, input_size=(n_observations, 84, 84))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgxRkryv1dYi",
        "outputId": "04612423-632a-4037-e4cc-32b8dda4c4ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 20, 20]           8,224\n",
            "       BatchNorm2d-2           [-1, 32, 20, 20]              64\n",
            "           Dropout-3           [-1, 32, 20, 20]               0\n",
            "            Conv2d-4             [-1, 64, 9, 9]          32,832\n",
            "       BatchNorm2d-5             [-1, 64, 9, 9]             128\n",
            "           Dropout-6             [-1, 64, 9, 9]               0\n",
            "            Conv2d-7             [-1, 64, 7, 7]          36,928\n",
            "       BatchNorm2d-8             [-1, 64, 7, 7]             128\n",
            "           Dropout-9             [-1, 64, 7, 7]               0\n",
            "           Linear-10                  [-1, 512]       1,606,144\n",
            "           Linear-11                    [-1, 6]           3,078\n",
            "================================================================\n",
            "Total params: 1,687,526\n",
            "Trainable params: 1,687,526\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.11\n",
            "Forward/backward pass size (MB): 0.49\n",
            "Params size (MB): 6.44\n",
            "Estimated Total Size (MB): 7.03\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install torchsummary\n",
        "pip install torchviz graphviz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAxjEOWI2QuY",
        "outputId": "ca2174a1-b0ce-42f3-912e-86f396217628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4131 sha256=98ddf64b859a259f2e53bfbbc989806388697030d18509b5d65207457e025ab3\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchviz import make_dot\n",
        "from graphviz import Digraph\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "0619bCuc2S36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(n_observations, 32, kernel_size=(8, 8), stride=4)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(4, 4), stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.dropout3 = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.layer1 = nn.Linear(3136, 512)\n",
        "        self.layer2 = nn.Linear(512, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.dropout3(x)\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = F.relu(self.layer1(x))\n",
        "        out = self.layer2(x)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "4aKiHac-2cwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming n_observations and n_actions are already defined\n",
        "model = DQN(n_observations, n_actions)\n",
        "example_input = torch.randn(1, n_observations, 84, 84)\n"
      ],
      "metadata": {
        "id": "F09Bo1TM2xRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the example input through the model\n",
        "output = model(example_input)\n",
        "\n",
        "# Create a computation graph visualization\n",
        "dot = make_dot(output, params=dict(model.named_parameters()))\n",
        "\n",
        "# Save the graph to a file (optional)\n",
        "dot.format = 'png'\n",
        "dot.render('dqn_model_graph', view=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "Xf1BWtcS2y30",
        "outputId": "dbe7a1dc-7814-49fc-9cfc-5cf489a60ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchviz/dot.py:65: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if LooseVersion(torch.__version__) < LooseVersion(\"1.9\") and \\\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dqn_model_graph.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchviz import make_dot\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(n_observations, 32, kernel_size=(8, 8), stride=4)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(4, 4), stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.dropout3 = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.layer1 = nn.Linear(3136, 512)\n",
        "        self.layer2 = nn.Linear(512, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.dropout3(x)\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = F.relu(self.layer1(x))\n",
        "        out = self.layer2(x)\n",
        "        return out\n",
        "\n",
        "# Assuming n_observations and n_actions are already defined\n",
        "model = DQN(n_observations, n_actions)\n",
        "example_input = torch.randn(1, n_observations, 84, 84)\n",
        "\n",
        "# Create a computation graph visualization without extending the convolutional layers\n",
        "dot = make_dot(model(example_input), params=dict(model.named_parameters()), show_attrs=True, show_saved=True)\n",
        "\n",
        "# Set the background color to black and render the graph\n",
        "dot.format = 'png'\n",
        "dot.attr(bgcolor='grey')\n",
        "dot.render('dqn_model_graph_dark', view=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "p6XkXUHL3kfp",
        "outputId": "1e402874-4bf7-40c0-a80b-1310a1b26c08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchviz/dot.py:65: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if LooseVersion(torch.__version__) < LooseVersion(\"1.9\") and \\\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dqn_model_graph_dark.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us_UDxNrrkBe"
      },
      "outputs": [],
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < MIN_MEMORY_LEN:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "\n",
        "\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "\n",
        "\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    with torch.no_grad():\n",
        "\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    #criterion = nn.CrossEntropyLoss()\n",
        "    #criterion = nn.MSELoss()\n",
        "\n",
        "    #loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # In-place gradient clipping\n",
        "    #torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    #torch.nn.utils.clip_grad_value_(policy_net.parameters(), 1)\n",
        "\n",
        "    optimizer.step()\n",
        "    #scheduler.step()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#new\n",
        "action_labels = {\n",
        "    0: 'NOOP',\n",
        "    1: 'FIRE',\n",
        "    2: 'RIGHT',\n",
        "    3: 'LEFT',\n",
        "    4: 'RIGHTFIRE',\n",
        "    5: 'LEFTFIRE'\n",
        "}\n",
        "def doplot(num):\n",
        "    #w=target_net.conv1.weight.detach().cpu()\n",
        "    #w=torch.nn.functional.normalize(w, dim=(1))\n",
        "\n",
        "    #img=make_grid(w,nrow=8,padding=2,normalize=True)\n",
        "    #img_numpy=img.permute(1,2,0).numpy()\n",
        "\n",
        "    last_actions = [int(transition.action[0]) for transition in memory.memory][-t:]\n",
        "    action_counts = [last_actions.count(action) for action in action_labels.keys()]\n",
        "\n",
        "\n",
        "\n",
        "    fig.suptitle(f'DQN Model on {GAMES}')\n",
        "    ax3.cla()\n",
        "    ax1.set_yscale('log')\n",
        "    ax1.plot(np.arange(len(average_loss)),average_loss)\n",
        "    ax1.set_ylabel(\"Average_Loss\")\n",
        "    ax2.plot(np.arange(len(main_reward)),main_reward,color='red')\n",
        "    ax2.set_ylabel(\"Reward\")\n",
        "    ax2.set_ylim(-22, 22)\n",
        "    ax3.bar(list(action_labels.values()), action_counts, width=0.6, color='purple')\n",
        "    ax3.tick_params(axis='x', labelsize=8, rotation=45)\n",
        "    ax3.set_ylabel('Counts')\n",
        "    ax4.plot([get_epsilon(DECAY_TYPE,i) for i in range(i_episode)])\n",
        "    ax4.set_ylabel(f'{DECAY_TYPE}_Decay_Epsilon')\n",
        "    plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
        "    fig.savefig(f\"{num}.png\")\n"
      ],
      "metadata": {
        "id": "PrSY6gxkUy-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#since training models takes so much time. We can save trained model using below functions\n",
        "#Sample Usage\n",
        "#modelfile=save_model(target_net,my_param)\n",
        "#load_model(model,modelfile)\n",
        "def save_checkpoint(state, modelname):\n",
        "    torch.save(state, modelname)\n",
        "\n",
        "def save_model(model,my_param):\n",
        "  save_checkpoint({\n",
        "  'epoch': i_episode,\n",
        "  'state_dict': model.state_dict(),\n",
        "  'optimizer' : optimizer.state_dict(),},\n",
        "  modelname=\"model_{}_ep{}_BS{}_LR{}_tau{}.pth\".format(my_param[\"Author\"],my_param[\"EPOCHS\"],my_param[\"BATCH_SIZE\"],my_param[\"LR\"],my_param[\"TAU\"]))\n",
        "\n",
        "def load_model(model,modelfile):\n",
        "  checkpoint=torch.load(modelfile)\n",
        "  model.load_state_dict(checkpoint['state_dict'])\n",
        "  optimizer =  modelfile[['optimizer']]\n",
        "  i_episode = modelfile[['epoch']]\n",
        "  LR = optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n",
        "  return model ,optimizer, i_episode, LR"
      ],
      "metadata": {
        "id": "MrET6UZQzLt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8jZ-xWprt0_"
      },
      "outputs": [],
      "source": [
        "timeframe = 0\n",
        "running_loss = []\n",
        "running_reward = []\n",
        "average_reward = []\n",
        "average_loss = []\n",
        "main_reward = []\n",
        "x_ax = np.arange(num_episodes)\n",
        "\n",
        "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(12, 6))\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    total_reward = 0\n",
        "    print(f\"episode: {i_episode}\")\n",
        "    start_time = time.time()\n",
        "    scheduler.step()\n",
        "    # Initialize the environment and get it's state\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    #state = resize_frame(old_state)\n",
        "    #state = np.stack((state, state, state, state))\n",
        "    #state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    for t in count():\n",
        "        action = select_action(state)\n",
        "        old_observation, reward, terminated, truncated, _ = env.step(action.item())\n",
        "        observation = resize_frame(old_observation)\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        done = terminated or truncated\n",
        "        state = state.squeeze(0)\n",
        "\n",
        "        if terminated:\n",
        "\n",
        "            next_state = torch.stack((state[0], state[1], state[2],state[3])).unsqueeze(0)\n",
        "\n",
        "        else:\n",
        "            next_state = torch.tensor(observation, dtype=torch.float32, device=device)\n",
        "            next_state = torch.stack((next_state, state[0], state[1], state[2])).unsqueeze(0)\n",
        "\n",
        "        # Move to the next state\n",
        "\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        memory.push(state, action, next_state, reward)\n",
        "\n",
        "        # Perform one step of the optimization (on the policy network)\n",
        "        loss = optimize_model()\n",
        "        if loss == None:\n",
        "          continue\n",
        "        else:\n",
        "\n",
        "          running_loss.append(float(loss.detach().cpu().numpy()))\n",
        "\n",
        "        # Soft update of the target network's weights\n",
        "        # θ′ ← τ θ + (1 −τ )θ′\n",
        "        if timeframe % UPDATE_FREQ == 0:\n",
        "          print(\"Updating the Target Network\")\n",
        "          target_net_state_dict = target_net.state_dict()\n",
        "          policy_net_state_dict = policy_net.state_dict()\n",
        "          for key in policy_net_state_dict:\n",
        "              target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "          target_net.load_state_dict(target_net_state_dict)\n",
        "        total_reward += float(reward.detach().cpu().numpy())\n",
        "\n",
        "        running_reward.append(total_reward)\n",
        "        timeframe = timeframe + 1\n",
        "\n",
        "        if done:\n",
        "\n",
        "          main_reward.append(np.mean(running_reward))\n",
        "          running_reward.clear()\n",
        "          break\n",
        "\n",
        "    avg_loss = np.mean(running_loss)\n",
        "    average_loss.append(avg_loss)\n",
        "    #new\n",
        "    if i_episode % plot_every ==0 :\n",
        "      doplot(i_episode)\n",
        "    elif i_episode % 530 == 0: #change 530 to any episodes you want\n",
        "      save_model(target_net,my_param)\n",
        "\n",
        "\n",
        "\n",
        "#new\n",
        "doplot(num_episodes)\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "print('Complete')\n",
        "print(f\"It takes {duration:.2f} for {num_episodes}\")\n",
        "\n",
        "my_param['Final_Loss'] = f\"{average_loss[-1]:.3g}\"\n",
        "my_param[\"Final_Reward\"] = main_reward[-1]\n",
        "my_param['time'] =f\"{duration:.2f}\"\n",
        "Save_as_csv(SAVE,my_param)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Decay See how the Epsilon changes as a function of Episodes"
      ],
      "metadata": {
        "id": "jIXvFlXidcfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_net.conv1.weight[:,1:3].shape"
      ],
      "metadata": {
        "id": "_oOWyFT5VTf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon_by_epsiode = lambda steps_done: max(EPS_START - (EPS_START - EPS_END) * (steps_done / EPS_DECAY),EPS_END)\n",
        "\n",
        "\n",
        "plt.plot([epsilon_by_epsiode(i) for i in range(num_episodes)])"
      ],
      "metadata": {
        "id": "g2dTYdQ-8QtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exponential Decay\n",
        "See how the Epsilon changes as a function of Episodes"
      ],
      "metadata": {
        "id": "KvKi5TXddS0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon_by_epsiode = lambda steps_done: EPS_END + (EPS_START - EPS_END) * math.exp(-2 * steps_done / EPS_DECAY)\n",
        "plt.plot([epsilon_by_epsiode(i) for i in range(500)])\n"
      ],
      "metadata": {
        "id": "r3gbWyfKdF2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwwyrByoPlez"
      },
      "outputs": [],
      "source": [
        "# @title Play Video function\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "# create the directo  ry to store the video(s)\n",
        "os.makedirs(\"./video/ALE/\", exist_ok=True)\n",
        "#display = Display(visible=False, size=(1400, 900))\n",
        "#_ = display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment\n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "def render_mp4(videopath: str) -> str:\n",
        "  \"\"\"\n",
        "  Gets a string containing a b4-encoded version of the MP4 video\n",
        "  at the specified path.\n",
        "  \"\"\"\n",
        "  mp4 = open(videopath, 'rb').read()\n",
        "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
        "  return f''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAfpGg5NQeJa"
      },
      "outputs": [],
      "source": [
        "env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "vid = VideoRecorder(env, path=f\"video/{env_name}_learned.mp4\")\n",
        "env = FireResetEnv(env)\n",
        "#env = MaxAndSkipEnv(env)\n",
        "old_state = env.reset()\n",
        "state = resize_frame(old_state)\n",
        "\n",
        "state = np.stack((state, state, state, state))\n",
        "\n",
        "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "total_reward = 0\n",
        "done = False\n",
        "while not done:\n",
        "  frame = env.render()\n",
        "  vid.capture_frame()\n",
        "\n",
        "  action = select_action(state)\n",
        "\n",
        "  old_observation, reward, done, truncated, _ = env.step(action.item())\n",
        "  observation = resize_frame(old_observation)\n",
        "\n",
        "  total_reward += reward\n",
        "vid.close()\n",
        "env.close()\n",
        "print(f\"\\nTotal reward: {total_reward}\")\n",
        "\n",
        "# show video\n",
        "html = render_mp4(f\"video/{env_name}_learned.mp4\")\n",
        "HTML(html)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exponential decay\n"
      ],
      "metadata": {
        "id": "GMpd9xj_TIMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### saving and loading model\n"
      ],
      "metadata": {
        "id": "W9N2XOE7zJSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, pathfile)"
      ],
      "metadata": {
        "id": "z6NJJE3CZrCb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}